{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LSTM_Model.__init__() got an unexpected keyword argument 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tianrui Ye\\Desktop\\Final\\code.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tianrui%20Ye/Desktop/Final/code.ipynb#W0sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m dataset \u001b[39m=\u001b[39m MyDataset(train_x, train_y, torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu:0\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tianrui%20Ye/Desktop/Final/code.ipynb#W0sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m data_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tianrui%20Ye/Desktop/Final/code.ipynb#W0sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     worker_init_fn\u001b[39m=\u001b[39mseed_worker) \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Tianrui%20Ye/Desktop/Final/code.ipynb#W0sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m model \u001b[39m=\u001b[39m LSTM_Model(num_features\u001b[39m=\u001b[39m\u001b[39m28169\u001b[39m, densenet_output_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_classes\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tianrui%20Ye/Desktop/Final/code.ipynb#W0sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Tianrui%20Ye/Desktop/Final/code.ipynb#W0sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: LSTM_Model.__init__() got an unexpected keyword argument 'num_features'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import optuna\n",
    "import torchvision\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DenseNet_CNN_Model(nn.Module):\n",
    "    def __init__(self, num_features, densenet_output_size, num_classes):\n",
    "        super(DenseNet_CNN_Model, self).__init__()\n",
    "\n",
    "        self.cnn_input_size = int(np.ceil(np.sqrt(num_features)))\n",
    "\n",
    "        self.densenet = torchvision.models.densenet121(pretrained=True)\n",
    "        self.densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        self.densenet.classifier = nn.Linear(self.densenet.classifier.in_features, densenet_output_size)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        cnn_output_size = 32 * ((self.cnn_input_size // 2) // 2) ** 2\n",
    "\n",
    "        self.fc = nn.Linear(densenet_output_size + cnn_output_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_padded = F.pad(input=x, pad=(0, self.cnn_input_size**2 - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        x_densenet = x_padded.view(x.size(0), 1, self.cnn_input_size, self.cnn_input_size)\n",
    "        x_cnn = x_padded.view(x.size(0), 1, self.cnn_input_size, self.cnn_input_size)\n",
    "\n",
    "        x_densenet = self.densenet(x_densenet)\n",
    "\n",
    "        x_cnn = F.relu(self.conv1(x_cnn))\n",
    "        x_cnn = self.pool(x_cnn)\n",
    "        x_cnn = F.relu(self.conv2(x_cnn))\n",
    "        x_cnn = self.pool(x_cnn)\n",
    "        x_cnn = x_cnn.view(x_cnn.size(0), -1) \n",
    "\n",
    "        x_combined = torch.cat((x_densenet, x_cnn), dim=1)\n",
    "\n",
    "        x_combined = self.fc(x_combined)\n",
    "        return x_combined\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# a helper class to load data, keep it as is\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, x, y, device):\n",
    "        self.x = torch.from_numpy(x).to(torch.float32)\n",
    "        self.y = torch.from_numpy(y).to(torch.long) \n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        xi = self.x[index].to(self.device)\n",
    "        yi = self.y[index].to(self.device)\n",
    "        return xi, yi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    This function is to ensure reproducibility\n",
    "\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "\n",
    "train_x = np.load('C:/Users/Tianrui Ye/Desktop/Final/train_x.npy', allow_pickle=True)\n",
    "train_y = np.load('C:/Users/Tianrui Ye/Desktop/Final/train_y.npy', allow_pickle=True)\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "torch.manual_seed(0)\n",
    " \n",
    "dataset = MyDataset(train_x, train_y, torch.device(\"cpu:0\"))\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True, \n",
    "    worker_init_fn=seed_worker) \n",
    "\n",
    "model = DenseNet_CNN_Model(num_features=28169, densenet_output_size=64, num_classes=3)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "for epoch in range(50):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(data_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        if i % 20 == 19:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_loss = running_loss / len(data_loader)\n",
    "    epoch_acc = correct / total\n",
    "    print(f'End of Epoch {epoch + 1}, Training Loss: {epoch_loss}, Training Accuracy: {epoch_acc}')\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6129807692307693\n",
      "Confusion Matrix:\n",
      "[[ 67  56   3]\n",
      " [ 41 106  30]\n",
      " [  9  22  82]]\n",
      "F1 Score: 0.6122759172447538\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55       126\n",
      "           1       0.58      0.60      0.59       177\n",
      "           2       0.71      0.73      0.72       113\n",
      "\n",
      "    accuracy                           0.61       416\n",
      "   macro avg       0.62      0.62      0.62       416\n",
      "weighted avg       0.61      0.61      0.61       416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "val_x_tensor = torch.from_numpy(val_x).to(torch.float32)\n",
    "val_y_tensor = torch.from_numpy(val_y).to(torch.long)\n",
    "\n",
    "val_dataset = TensorDataset(val_x_tensor, val_y_tensor)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval() \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "            actuals.extend(labels.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    conf_matrix = confusion_matrix(actuals, predictions)\n",
    "    f1 = f1_score(actuals, predictions, average='weighted') \n",
    "    report = classification_report(actuals, predictions)\n",
    "\n",
    "    return accuracy, conf_matrix, f1, report\n",
    "\n",
    "accuracy, conf_matrix, f1, report = evaluate_model(model, val_loader)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-22 16:02:58,724] A new study created in memory with name: no-name-f6f65961-35c9-487c-9172-dc4c58fdd985\n",
      "[W 2023-11-22 16:02:59,028] Trial 0 failed with parameters: {'lr': 0.0544601856276853, 'batch_size': 16, 'n_hidden_1': 454, 'n_hidden_2': 403, 'dropout_p': 0.42424439269086356} because of the following error: TypeError('empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tianrui Ye\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Tianrui Ye\\AppData\\Local\\Temp\\ipykernel_32700\\2618736915.py\", line 92, in objective\n",
      "    model = ImprovedLinear(train_x.shape[1], n_hidden_1, n_hidden_2, 3, dropout_p)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Tianrui Ye\\AppData\\Local\\Temp\\ipykernel_32700\\2618736915.py\", line 24, in __init__\n",
      "    self.layer4 = nn.Linear(n_hidden_3, out_dim)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Apps\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 96, in __init__\n",
      "    self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n",
      " * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      " * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
      "\n",
      "[W 2023-11-22 16:02:59,030] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-3.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=119'>120</a>\u001b[0m \u001b[39m# 创建一个study对象，并运行优化\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=120'>121</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=121'>122</a>\u001b[0m study\u001b[39m.\u001b[39moptimize(objective, n_trials\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)  \u001b[39m# 您可以根据需要调整 n_trials 的值\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=123'>124</a>\u001b[0m \u001b[39m# 最佳超参数\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=124'>125</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(catch) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(catch, Iterable) \u001b[39melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32mUntitled-3.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=88'>89</a>\u001b[0m val_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(val_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=90'>91</a>\u001b[0m \u001b[39m# 初始化模型\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=91'>92</a>\u001b[0m model \u001b[39m=\u001b[39m ImprovedLinear(train_x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], n_hidden_1, n_hidden_2, \u001b[39m3\u001b[39m, dropout_p)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=92'>93</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlr)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=93'>94</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[1;32mUntitled-3.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=21'>22</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(n_hidden_2, n_hidden_3)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=22'>23</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBatchNorm1d(n_hidden_3)  \u001b[39m# 添加批量归一化\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=23'>24</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(n_hidden_3, out_dim)\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=24'>25</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU()\n\u001b[0;32m     <a href='vscode-notebook-cell:Untitled-3.ipynb?jupyter-notebook#X10sdW50aXRsZWQ%3D?line=25'>26</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39mdropout_p)\n",
      "File \u001b[1;32md:\\Apps\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((out_features, in_features), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import optuna\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "class ImprovedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, \n",
    "                 n_hidden_1, n_hidden_2, n_hidden_3,\n",
    "                 out_dim, dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, n_hidden_1)\n",
    "        self.bn1 = nn.BatchNorm1d(n_hidden_1)  # 添加批量归一化\n",
    "        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.bn2 = nn.BatchNorm1d(n_hidden_2)  # 添加批量归一化\n",
    "        self.layer3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.bn3 = nn.BatchNorm1d(n_hidden_3)  # 添加批量归一化\n",
    "        self.layer4 = nn.Linear(n_hidden_3, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# a helper class to load data, keep it as is\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, x, y, device):\n",
    "        self.x = torch.from_numpy(x).to(torch.float32)\n",
    "        self.y = torch.from_numpy(y).to(torch.long)  # Convert labels to Long type\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        xi = self.x[index].to(self.device)\n",
    "        yi = self.y[index].to(self.device)\n",
    "        return xi, yi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    \"\"\"\n",
    "    This function is to ensure reproducibility\n",
    "\n",
    "    \"\"\"\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "    n_hidden_1 = trial.suggest_int(\"n_hidden_1\", 100, 500)\n",
    "    n_hidden_2 = trial.suggest_int(\"n_hidden_2\", 100, 500)\n",
    "    dropout_p = trial.suggest_float(\"dropout_p\", 0.2, 0.5)\n",
    "\n",
    "    train_x = np.load('C:/Users/Tianrui Ye/Desktop/Final/train_x.npy', allow_pickle=True)\n",
    "    train_y = np.load('C:/Users/Tianrui Ye/Desktop/Final/train_y.npy', allow_pickle=True)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = MyDataset(train_x, train_y, torch.device(\"cpu\"))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker)\n",
    "\n",
    "    val_dataset = MyDataset(val_x, val_y, torch.device(\"cpu\"))\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = ImprovedLinear(train_x.shape[1], n_hidden_1, n_hidden_2, 3, dropout_p)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50) \n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('This', 'is'): {'a': 1, 'used': 1}, ('is', 'a'): {'sample': 1}, ('a', 'sample'): {'training': 1}, ('sample', 'training'): {'text': 1}, ('training', 'text'): {'.': 1}, ('text', '.'): {'This': 1}, ('.', 'This'): {'is': 1}, ('is', 'used'): {'to': 1}, ('used', 'to'): {'demonstrate': 1}, ('to', 'demonstrate'): {'the': 1}, ('demonstrate', 'the'): {'text': 1}, ('the', 'text'): {'generation': 1}, ('text', 'generation'): {'.': 1}}\n",
      "['training', 'text']\n",
      "training text . This is a sample training text . This is a sample training text .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "# bi-gram model\n",
    "def get_counts(context_length, training_text): # context_length:2  training_text: \"This is a sample training text. This is used to demonstrate the text generation.\"\n",
    "    counts = {}\n",
    "    tokens = word_tokenize(training_text)\n",
    "\n",
    "    for i in range(len(tokens) - context_length): # i: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14\n",
    "        context = tokens[i:i + context_length] # 0:2 -> 0,1\n",
    "        next_token = tokens[i + context_length]\n",
    "        \n",
    "        context_tuple = tuple(context)\n",
    "        if context_tuple in counts:\n",
    "            if next_token in counts[context_tuple]:\n",
    "                counts[context_tuple][next_token] += 1\n",
    "            else:\n",
    "                counts[context_tuple][next_token] = 1 # \"This is\" -> {\"a\": 1, \"used\": 1}\n",
    "        else:\n",
    "            counts[context_tuple] = {next_token: 1} # key -> value\n",
    "    \n",
    "    return counts\n",
    "\n",
    "def generate_from_file(context_length, training_text, output_length=10):\n",
    "    counts = get_counts(context_length, training_text)\n",
    "\n",
    "    first_tokens = random.choice(list(counts.keys())) # \"This is\"\n",
    "    output_list = list(first_tokens) # [\"This\", \"is\"]\n",
    "    current_context = first_tokens # (\"This\", \"is\") -> \"a\"    (\"is\",\"a\") -> \"sample\"\n",
    "\n",
    "    for i in range(output_length): # 0~10\n",
    "        if current_context in counts and counts[current_context] is not None:\n",
    "            next_token = max(counts[current_context], key=counts[current_context].get)\n",
    "            temp = list(current_context[1:]) + [next_token]\n",
    "            current_context = tuple(temp)\n",
    "            output_list.append(next_token)\n",
    "\n",
    "    return \" \".join(output_list)\n",
    "\n",
    "# Example usage\n",
    "training_text = \"This is a sample training text. This is used to demonstrate the text generation.\"\n",
    "output = generate_from_file(2, training_text, 15)\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
